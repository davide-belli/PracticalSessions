{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Version_EEML_RL_Tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "9v_SYckYfv5G",
        "rNuohp44N00i",
        "ztQEQvnKh2t6",
        "ALrRR76eAd6u",
        "cOu9RZY3AkF1",
        "B8oKd0oyvNcH",
        "iJastp_kcAZC"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H60DVrDg1_Gq",
        "colab_type": "text"
      },
      "source": [
        "#RL Tutorial \n",
        "\n",
        "Designed for education purposes. Please do not distribute without explicit permission.\n",
        "\n",
        "\n",
        "**Questions/Correspondence**: (borsa@google.com)\n",
        "\n",
        "*Special thanks to*: Hado van Hasselt, Matteo Hessel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v_SYckYfv5G",
        "colab_type": "text"
      },
      "source": [
        "## Content\n",
        "\n",
        "In this tutorial, we will investigate the properties of 4 distinct reinforcement learning algorithms:\n",
        "\n",
        "* Policy Evaluation\n",
        "* Online Control: SARSA, Q-learning\n",
        "* Experience Replay\n",
        "* REINFORCE Algorithm \n",
        "\n",
        "Some dimensions of the RL problems we will be considering:\n",
        "* Tabular vs Function Approximation\n",
        "* Off-policy/On-policy Control\n",
        "* Online vs Replay\n",
        "* Exploration vs Explotation\n",
        "\n",
        "\n",
        "## Background Reading and Reference Material\n",
        "\n",
        "* Sutton and Barto (2018), Chapters 3-8\n",
        "* RL Basics lecture notes (@Doina)\n",
        "* Review RL slides (@Diana)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNuohp44N00i",
        "colab_type": "text"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "You will use Python to implement several reinforcement learning algorithms. Only the second part will deal with functional approximation for which we will be using neural networks to approximate value functions and policies.\n",
        "\n",
        "You will then run these algorithms on a few problems, to understand their properties and different emerging behaviour. In this tutorial we will focus primary on fundamental algorithms in RL and explore them in a simple gridworld setting. That being said, these are algorithms that have now been shown to scale very well with (non-linear) functional approximations. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztQEQvnKh2t6",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps5OnkPmDbMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "import itertools\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=1)\n",
        "plt.style.use('seaborn-notebook')\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALrRR76eAd6u",
        "colab_type": "text"
      },
      "source": [
        "## Environments: Grid-Worlds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMC6nODK1HAV",
        "colab_type": "text"
      },
      "source": [
        "**(Simple) Tabular Grid-World**\n",
        "\n",
        "You can visualize the grid worlds we will train our agents on, by running the cells below.\n",
        "`S` indicates the start state and `G` indicates the goal.  The agent has four possible actions: up, right, down, and left.  Rewards are: `-5` for bumping into a wall, `+10` for reaching the goal, and `0` otherwise.  The episode ends when the agent reaches the goal, and otherwise continues.  The discount, on continuing steps, is $\\gamma = 0.9$.\n",
        "\n",
        "We will use three distinct GridWorlds:\n",
        "* `Grid` tabular grid world with a goal in the top right of the grid\n",
        "* `AltGrid` tabular grid world with a goal in the bottom left of the grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP97bVN3NuG8",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Environment: Gridworld Implementation\n",
        "class Grid(object):\n",
        "\n",
        "  def __init__(self, discount=0.9, penalty_for_walls=-5):\n",
        "    # -1: wall\n",
        "    # 0: empty, episode continues\n",
        "    # other: number indicates reward, episode will terminate\n",
        "    self._layout = np.array([\n",
        "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "      [-1,  0,  0,  0,  0,  0, -1,  0,  0, -1],\n",
        "      [-1,  0,  0,  0, -1,  0,  0,  0, 10, -1],\n",
        "      [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
        "    ])\n",
        "    self._start_state = (2, 2)\n",
        "    self._goal_state = (8, 2)\n",
        "    self._state = self._start_state\n",
        "    self._number_of_states = np.prod(np.shape(self._layout))\n",
        "    self._discount = discount\n",
        "    self._penalty_for_walls = penalty_for_walls\n",
        "    self._layout_dims = self._layout.shape\n",
        "\n",
        "  @property\n",
        "  def number_of_states(self):\n",
        "      return self._number_of_states\n",
        "    \n",
        "  def plot_grid(self):\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    plt.imshow(self._layout <= -1, interpolation=\"nearest\")     \n",
        "    ax = plt.gca()\n",
        "    ax.grid(0)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(\"The grid\")\n",
        "    plt.text(\n",
        "        self._start_state[0], self._start_state[1], \n",
        "        r\"$\\mathbf{S}$\", ha='center', va='center')\n",
        "    plt.text(\n",
        "        self._goal_state[0], self._goal_state[1], \n",
        "        r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "    h, w = self._layout.shape\n",
        "    for y in range(h-1):\n",
        "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
        "    for x in range(w-1):\n",
        "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n",
        "\n",
        "  \n",
        "  def get_obs(self):\n",
        "    y, x = self._state\n",
        "    return y*self._layout.shape[1] + x\n",
        "  \n",
        "  def int_to_state(self, int_obs):\n",
        "    x = int_obs % self._layout.shape[1]\n",
        "    y = int_obs // self._layout.shape[1]\n",
        "    return y, x\n",
        "\n",
        "  def step(self, action):\n",
        "    y, x = self._state\n",
        "\n",
        "    if action == 0:  # up\n",
        "      new_state = (y - 1, x)\n",
        "    elif action == 1:  # right\n",
        "      new_state = (y, x + 1)\n",
        "    elif action == 2:  # down\n",
        "      new_state = (y + 1, x)\n",
        "    elif action == 3:  # left\n",
        "      new_state = (y, x - 1)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
        "\n",
        "    new_y, new_x = new_state\n",
        "    if self._layout[new_y, new_x] == -1:  # wall\n",
        "      reward = self._penalty_for_walls\n",
        "      discount = self._discount\n",
        "      new_state = (y, x)\n",
        "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
        "      reward = 0.\n",
        "      discount = self._discount\n",
        "    else:  # a goal\n",
        "      reward = self._layout[new_y, new_x]\n",
        "      discount = 0.\n",
        "      new_state = self._start_state\n",
        "    \n",
        "    self._state = new_state\n",
        "    return reward, discount, self.get_obs()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXyPvOq-S2OT",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Alternative Environment: Gridworld (with different goal state)\n",
        "class AltGrid(Grid):\n",
        "  \n",
        "    def __init__(self, discount=0.9, penalty_for_walls=-5):\n",
        "      # -1: wall\n",
        "      # 0: empty, episode continues\n",
        "      # other: number indicates reward, episode will terminate\n",
        "      self._layout = np.array([\n",
        "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "        [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "        [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
        "        [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
        "        [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
        "        [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "        [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "        [-1,  0, 10,  0,  0,  0,  0,  0,  0, -1],\n",
        "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
        "      ])\n",
        "      self._start_state = (2, 2)\n",
        "      self._goal_state = (2, 7)\n",
        "      self._state = self._start_state\n",
        "      self._number_of_states = np.prod(np.shape(self._layout))\n",
        "      self._discount = discount\n",
        "      self._penalty_for_walls = penalty_for_walls\n",
        "      self._layout_dims = self._layout.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVUhh2qqwep_",
        "colab_type": "code",
        "outputId": "07167ce4-32e0-4ebc-a44a-7ad9a339a9f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "# Visualise the two environments\n",
        "\n",
        "# Instantiate the two tabular environments\n",
        "grid = Grid()\n",
        "alt_grid = AltGrid()\n",
        "\n",
        "# Plot tabular environments\n",
        "grid.plot_grid()\n",
        "alt_grid.plot_grid()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAALUAAAC0CAYAAADSD20MAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAALUAAAC0CAYAAADSD20MAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJnH+BSrDbBwh2CXCAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbEydMqDKxZr",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Policies (Uniformly random and e-greedy) \n",
        "#Expected syntax: `policy(q_values)` \n",
        "\n",
        "# uniformly random policy\n",
        "def random_policy(q):\n",
        "  return np.random.randint(4)\n",
        "\n",
        "# epilson-greedy policy\n",
        "def epsilon_greedy(q_values, epsilon=0.1):\n",
        "  if epsilon < np.random.random():\n",
        "    return np.argmax(q_values)\n",
        "  else:\n",
        "    return np.random.randint(np.array(q_values).shape[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOu9RZY3AkF1",
        "colab_type": "text"
      },
      "source": [
        "## Helper functions (for visualization and running experiments)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EttQGJ1n5Zn",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Helper functions for visualisation\n",
        "\n",
        "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
        "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
        "\n",
        "def plot_values(values, colormap='pink', vmin=-1, vmax=10):\n",
        "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  plt.colorbar(ticks=[vmin, vmax])\n",
        "\n",
        "# Visualising the e-greedy value function V^{\\mu}\n",
        "def plot_state_value(action_values, epsilon=0.1):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "  vmin = np.min(action_values)\n",
        "  vmax = np.max(action_values)\n",
        "  v = (1 - epsilon) * np.max(q, axis=-1) + epsilon * np.mean(q, axis=-1)\n",
        "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$d(s)$\")\n",
        "\n",
        "# Visualising all value functions, for all actions \n",
        "def plot_action_values(action_values, epsilon=0.1):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(8, 8))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "  vmin = np.min(action_values)\n",
        "  vmax = np.max(action_values)\n",
        "  dif = vmax - vmin\n",
        "  for a in [0, 1, 2, 3]:\n",
        "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
        "    \n",
        "    plot_values(q[..., a], vmin=vmin - 0.05*dif, vmax=vmax + 0.05*dif)\n",
        "    action_name = map_from_action_to_name(a)\n",
        "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
        "    \n",
        "  plt.subplot(3, 3, 5)\n",
        "  v = (1 - epsilon) * np.max(q, axis=-1) + epsilon * np.mean(q, axis=-1)\n",
        "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$v(s)$\")\n",
        "      \n",
        "  \n",
        "def smooth(x, window=10):\n",
        "  return x[:window*(len(x)//window)].reshape(len(x)//window, window).mean(axis=1)\n",
        "  \n",
        "\n",
        "def plot_stats(stats, window=10):\n",
        "  plt.figure(figsize=(16,4))\n",
        "  plt.subplot(121)\n",
        "  xline = range(0, len(stats.episode_lengths), window)\n",
        "  plt.plot(xline, smooth(stats.episode_lengths, window=window))\n",
        "  plt.ylabel('Episode Length')\n",
        "  plt.xlabel('Episode Count')\n",
        "  plt.subplot(122)\n",
        "  plt.plot(xline, smooth(stats.episode_rewards, window=window))\n",
        "  plt.ylabel('Episode Return')\n",
        "  plt.xlabel('Episode Count')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U86SLdA25GAY",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title [IMPORTANT] Running the experiments\n",
        "\n",
        "# Simple interaction loop with the MDP:\n",
        "# 1) Interact with the environment\n",
        "# 2) Agent gets observation, rewards, and discount from env. \n",
        "# and is expected to produce the next action\n",
        "def run_experiment(env, agent, number_of_steps):\n",
        "    mean_reward = 0.\n",
        "    try:\n",
        "      action = agent.initial_action()\n",
        "    except AttributeError:\n",
        "      action = 0\n",
        "      \n",
        "    # Interaction wih the MDP\n",
        "    for i in range(number_of_steps):\n",
        "      reward, discount, next_state = env.step(action)\n",
        "      action = agent.step(reward, discount, next_state)\n",
        "      mean_reward += (reward - mean_reward)/(i + 1.)\n",
        "\n",
        "    return mean_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWmvMHR5gM6N",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Helper functions for visualizing policies\n",
        "def plot_policy(grid, policy):\n",
        "  action_names = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
        "  grid.plot_grid()\n",
        "  plt.title('Policy Visualization')\n",
        "  for i in range(9):\n",
        "    for j in range(10):\n",
        "      action_name = action_names[policy[i,j]]\n",
        "      plt.text(j, i, action_name, ha='center', va='center')\n",
        "\n",
        "def plot_greedy_policy(grid, q):\n",
        "  action_names = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
        "  greedy_actions = np.argmax(q, axis=2)\n",
        "  grid.plot_grid()\n",
        "  plt.title('Greedy Policy')\n",
        "  for i in range(9):\n",
        "    for j in range(10):\n",
        "      action_name = action_names[greedy_actions[i,j]]\n",
        "      plt.text(j, i, action_name, ha='center', va='center')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzpb_dGVjT0O",
        "colab_type": "text"
      },
      "source": [
        "# RL Lab - Part 1: Tabular Agents\n",
        "\n",
        "Each agent, should implement a step function:\n",
        "\n",
        "### `__init__(self, number_of_actions, number_of_states, initial_observation)`:\n",
        "The constructor will provide the agent the number of actions, number of states, and the initial observation. You can get the initial observation by first instatiating an environment, using `grid = Grid()`, and then calling `grid.get_obs()`.\n",
        "\n",
        "Note: All agents should be in pure Python, not TensorFlow needed for this part.\n",
        "\n",
        "### `step(self, reward, discount, next_observation, ...)`:\n",
        "where `...` indicates there could be other inputs (discussed below).  The step should update the internal values, and return a new action to take.\n",
        "\n",
        "When the discount is zero ($\\text{discount} = \\gamma = 0$), then the `next_observation` will be the initial observation of the next episode.  One shouldn't bootstrap on the value of this state, which can simply be guaranteed when using \"$\\gamma \\cdot v(\\text{next_observation})$\" (for whatever definition of $v$ is appropriate) in the update, because $\\gamma = 0$.  So, the end of an episode can be seamlessly handled with the same step function.\n",
        "\n",
        "### `q_values()`:\n",
        "\n",
        "Tabular agents implement a function `q_values()` returning a matrix of Q values of shape: (`number_of_states`, `number_of_actions`)\n",
        "\n",
        "\n",
        "### A note on the initial action\n",
        "Normally, you would also have to implement a method that gives the initial action, based on the initial state.  As a convention, we will use the action `0` (which corresponds to `up`) as initial action.  Note that this initial action is only executed once, and the beginning of the first episode---not at the beginning of each episode.\n",
        "\n",
        "Q-learning and it's variants needs to remember the last action in order to update its value when they see the next state.  In the `__init__`, make sure you set the initial action to zero, e.g.,\n",
        "```\n",
        "def __init__(...):\n",
        "  (...)\n",
        "  self._action = 0\n",
        "  (...)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8oKd0oyvNcH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 1.0: Overview\n",
        "\n",
        "We are going to implement:\n",
        "- Prediction problem: Policy Evaluation\n",
        "- Towards control: Greedy Improvement \n",
        "- Online Tabular SARSA Agent\n",
        "- Online Tabular Q-learning Agent\n",
        "- Tabular Experience Replay Q-learning Agent\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XEP4mf4Jx70",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 1.1: Policy evaluation and Greedy Improvement\n",
        "\n",
        "The purpose here is to evaluate a given policy $\\pi$ -- compute the value function assoicated with following/employing this policy in a given MDP.\n",
        "\n",
        "$$ Q^{\\pi}(S,A) = \\mathbb{E}_{\\tau \\sim P^{\\pi}} [\\sum_t \\gamma^t R_t| s_0=s,a=a_0]$$\n",
        "\n",
        "where $\\tau = \\{s_0, a_0, r_0, s_1, a_1, r_1, \\cdots \\}$\n",
        "\n",
        "\n",
        "Algorithm:\n",
        "\n",
        "**Initialize** $Q(s, a)$ for all s ∈ $\\mathcal{S}$ and a ∈ $\\mathcal{A}(s)$\n",
        "\n",
        "**Loop forever**:\n",
        "\n",
        "1. $S \\gets{}$current (nonterminal) state\n",
        " \n",
        "2. $A \\gets{} \\text{behaviour_policy}(S)$\n",
        " \n",
        "3. Take action $A$; observe resulting reward $R$, discount $\\gamma$, and state, $S'$\n",
        "\n",
        "4. $Q(S, A) \\gets Q(S, A) + \\alpha (R + \\gamma Q(S', \\pi(S')) − Q(S, A))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nORJvcHML9os",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# uniformly random policy\n",
        "def random_policy(q):\n",
        "  return np.random.randint(4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IWIHIvxyC-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title [Coding Task] Policy Evaluation AGENT\n",
        "class PolicyEval_AGENT(object):\n",
        "\n",
        "  def __init__(\n",
        "      self, number_of_states, number_of_actions, initial_state, evaluated_policy, \n",
        "      behaviour_policy=random_policy, step_size=0.1):\n",
        "    self._action = 0\n",
        "    self._state = initial_state\n",
        "    self._number_of_states = number_of_states\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._step_size = step_size\n",
        "    self._behaviour_policy = behaviour_policy\n",
        "    self._evaluated_policy = evaluated_policy\n",
        "    \n",
        "    # ============ YOUR CODE HERE =============\n",
        "    # initialize your q-values (this is a table of state and action pairs\n",
        "    # Note: this can be random, but the code was tested w/ zero-initialization \n",
        "    # self._q =\n",
        "    pass\n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    pass\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    g = discount\n",
        "    next_s = next_state\n",
        "    \n",
        "    # ============ YOUR CODE HERE =============\n",
        "    # Q-value table update\n",
        "    pass\n",
        "  \n",
        "    # Get the action to send to execute in the environment and return it\n",
        "    pass\n",
        "  \n",
        "    return self._action\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCPr9KzBtFJ3",
        "colab_type": "text"
      },
      "source": [
        "**Task 1**: Run the policy evaluation agent, evaluating the uniformly randon policy  on the AltGrid() environment for $\\texttt{num_steps} = 1e3, 1e5$. \n",
        "\n",
        "Visualise the resulting value functions $Q(s,a)$. Plotting function is provided for you and it takes in a table of q-values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCWq1yKWp76Y",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "7b9c82b6-2393-4405-bf59-a73698cabffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "num_steps = int(1e5) # @param\n",
        "\n",
        "# environment\n",
        "grid = AltGrid()\n",
        "\n",
        "# agent \n",
        "agent = PolicyEval_AGENT(\n",
        "    number_of_states=grid._layout.size, \n",
        "    number_of_actions=4, \n",
        "    initial_state=grid.get_obs(),\n",
        "    evaluated_policy=random_policy,\n",
        "    behaviour_policy=random_policy,\n",
        "    step_size=0.1)\n",
        "\n",
        "# run experiment and get the value functions from agent\n",
        "run_experiment(grid, agent, num_steps)\n",
        "\n",
        "# get the q-values\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "\n",
        "# visualize value functions\n",
        "print('AFTER {} STEPS ...'.format(num_steps))\n",
        "plot_action_values(q)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-acf2f0b04237>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# get the q-values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# visualize value functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reshape'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN-KBoc2w9Na",
        "colab_type": "text"
      },
      "source": [
        "**Task 2: [Greedy Policy Improvement]** Compute and Visualise the greedy policy based on the above evaluation, at the end of the training process for $\\texttt{num_steps} = 1e5$.\n",
        "\n",
        "\n",
        "$$ \\pi_{greedy} (a|s) = \\arg\\max_a Q^{\\mu}(s,a) $$\n",
        "\n",
        "\n",
        "**Q: ** What do you observe? (Remember that we are evaluating the uniformly random policy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RbYQ2xmMBiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title [Coding Task] epilson-greedy policy\n",
        "# Input(s): Q(s,:), epsilon\n",
        "# Output:   Sampled action based on epsilon-Greedy(Q(s,:))\n",
        "def epsilon_greedy(q_values, epsilon=0.1):\n",
        "  pass\n",
        "  #return the epsilon greedy action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5XBEXuqufcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize the greedy policy (whatever works for you, \n",
        "# but you should be able to see what the agent would do\n",
        "# at each step/state)  \n",
        "\n",
        "pi = np.zeros(grid._layout_dims, dtype=np.int32)\n",
        "for i in range(grid._layout_dims[0]):\n",
        "  for j in range(grid._layout_dims[1]):\n",
        "    pi[i, j] = epsilon_greedy(q[i, j], epsilon=0.)\n",
        "    \n",
        "plot_policy(grid, pi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hLuA4e_zaiW",
        "colab_type": "text"
      },
      "source": [
        "**Task 3**: Re-run the same experiment: policy evaluation agent on the Grid() environment for $\\texttt{num_steps} = 1e5$\n",
        "and visualise the resulting value functions and the greedy policy on top of these values at the end of training.\n",
        "\n",
        "**Q: ** What do you observe? \n",
        "- How does this policy compare with the optimal one?\n",
        "- Try running the training process longer -- what do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZS4CfLtzaiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_steps = int(1e5) # @param\n",
        "\n",
        "grid = Grid()\n",
        "\n",
        "agent = PolicyEval_AGENT(\n",
        "    number_of_states=grid._layout.size, \n",
        "    number_of_actions=4, \n",
        "    initial_state=grid.get_obs(),\n",
        "    evaluated_policy=random_policy,\n",
        "    behaviour_policy=random_policy,\n",
        "    step_size=0.1)\n",
        "\n",
        "# run experiment and get the value functions from agent\n",
        "run_experiment(grid, agent, num_steps)\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "\n",
        "# visualise value functions\n",
        "plot_action_values(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1cC9EZJzaia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualise the greedy policy\n",
        "plot_greedy_policy(grid, q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbODeOkjyUEm",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 On-policy control: SARSA Agent\n",
        "In the following we are going to be concern with the control problem -- inferring the optimal value/policy that will 'solve' the MDP. The first algorithm we are going to be looking at is SARSA. \n",
        "\n",
        "Note: This is an **on-policy algoritm** -- i.e: the data collection is done on-policy.\n",
        "\n",
        "\n",
        "**Initialize** $Q(s, a)$ for all s ∈ S and a ∈ A(s)\n",
        "\n",
        "**Loop forever**:\n",
        "\n",
        "1. $S \\gets{}$current (nonterminal) state\n",
        " \n",
        "2. $A \\gets{} \\text{current_policy}(S)$\n",
        " \n",
        "3. Take action $A$; observe resultant reward $R$, discount $\\gamma$, and state, $S'$\n",
        "\n",
        "4. $Q(S, A) \\gets Q(S, A) + \\alpha (R + \\gamma Q(S', A') − Q(S, A))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g1cXTvXjJ7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title [Coding Task] SARSA Agent\n",
        "class SARSA_AGENT(object):\n",
        "\n",
        "  def __init__(\n",
        "      self, number_of_states, number_of_actions, initial_state, \n",
        "      behaviour_policy, num_offline_updates=0, step_size=0.1):\n",
        "    self._q = np.zeros((number_of_states, number_of_actions))\n",
        "    self._state = initial_state\n",
        "    self._number_of_states = number_of_states\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._step_size = step_size\n",
        "    self._behaviour_policy = behaviour_policy\n",
        "    self._action = 0\n",
        "    self._replay_buffer = []\n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    return self._q\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    g = discount\n",
        "    next_s = next_state\n",
        "    \n",
        "    # ============ YOUR CODE HERE =============\n",
        "    # Q-value table update\n",
        "    # td_error =\n",
        "    # self._q[s, a] =\n",
        "    pass\n",
        "  \n",
        "    # Get the action to send to execute in the environment and return it\n",
        "    self._state = next_state\n",
        "    self._action = self._behaviour_policy(self._q[next_state])\n",
        "    \n",
        "    return self._action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDjmJOD161Hx",
        "colab_type": "text"
      },
      "source": [
        "**Task**: Run the SARSA agent with different levels of exploration. \n",
        "\n",
        "- Moderate exploration: $\\texttt{epsilon} = 0.1$. \n",
        "\n",
        "- Very exploratory strategy: $\\texttt{epsilon} = 0.5, 1.0$.\n",
        "\n",
        "**Q**: Which do you expect, without running the experiment, to do better?  \n",
        "\n",
        " **Run multiple times**: What do you observe? ($\\texttt{epsilon} = 0.1$)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRHL8d6Q5ua5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epsilon = 0.1 #@param\n",
        "num_steps = int(1e5) #@param\n",
        "\n",
        "grid = Grid(discount=0.9, penalty_for_walls=-1.)\n",
        "\n",
        "behavior_policy = lambda qval: epsilon_greedy(qval, epsilon=epsilon)\n",
        "agent = SARSA_AGENT(\n",
        "    number_of_states=grid._layout.size, \n",
        "    number_of_actions=4, \n",
        "    initial_state=grid.get_obs(),\n",
        "    behaviour_policy=behavior_policy,\n",
        "    step_size=0.1)\n",
        "\n",
        "# run experiment and get the value functions from agent\n",
        "run_experiment(grid, agent, num_steps)\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "\n",
        "# visualize value functions\n",
        "print('AFTER {} STEPS ...'.format(num_steps))\n",
        "plot_action_values(q, epsilon=epsilon)\n",
        "\n",
        "# visualise the greedy policy\n",
        "plot_greedy_policy(grid, q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxjNg2_O-Dct",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Off-policy control: Q-learning Agent\n",
        "\n",
        "Reminder: Q-learning is a very powerful and general algorithm, that enable control (figuring out the optimal policy/value function) both on and off-policy.\n",
        "\n",
        "**Initialize** $Q(s, a)$ for all s ∈ S and a ∈ A(s)\n",
        "\n",
        "**Loop forever**:\n",
        "\n",
        "1. $S \\gets{}$current (nonterminal) state\n",
        " \n",
        "2. $A \\gets{} \\text{behaviour_policy}(S)$\n",
        " \n",
        "3. Take action $A$; observe resultant reward $R$, discount $\\gamma$, and state, $S'$\n",
        "\n",
        "4. $Q(S, A) \\gets Q(S, A) + \\alpha (R + \\gamma \\max_a Q(S', a) − Q(S, A))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9MR0wQ-jS_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title [Coding Task] Q-learning AGENT\n",
        "class QLearning_AGENT(object):\n",
        "\n",
        "  def __init__(\n",
        "      self, number_of_states, number_of_actions, initial_state, \n",
        "      behaviour_policy, num_offline_updates=0, step_size=0.1):\n",
        "    self._q = np.zeros((number_of_states, number_of_actions))\n",
        "    self._state = initial_state\n",
        "    self._action = 0\n",
        "    self._number_of_states = number_of_states\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._step_size = step_size\n",
        "    self._behaviour_policy = behaviour_policy\n",
        "    pass\n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    pass\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    g = discount\n",
        "    next_s = next_state\n",
        "    \n",
        "    # ============ YOUR CODE HERE =============\n",
        "    # Q-value table update\n",
        "    pass\n",
        "  \n",
        "    # Get the action to send to execute in the environment and return it\n",
        "    self._state = next_state\n",
        "    self._action = self._behaviour_policy(self._q[next_state])\n",
        "    \n",
        "    return self._action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cqpXmZta4l-",
        "colab_type": "text"
      },
      "source": [
        "### Task 1: Run your Q-learning agent on the below environment for 1e5 number of steps\n",
        "Keep the rest, to the default values, for this first step. I'll get to experiment with this later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgtCWNNcZJwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# enviroment\n",
        "grid = Grid(discount=0.9, penalty_for_walls=-1.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hduGg4l19acj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epsilon = 1.0 #@param\n",
        "num_steps = int(1e5) #@param\n",
        "\n",
        "# behavior policy\n",
        "behavior_policy = lambda qval: epsilon_greedy(qval, epsilon=epsilon)\n",
        "\n",
        "# agent\n",
        "agent = Qlearning_AGENT(\n",
        "    number_of_states=grid._layout.size, \n",
        "    number_of_actions=4, \n",
        "    initial_state=grid.get_obs(),\n",
        "    behaviour_policy=behavior_policy,\n",
        "    step_size=0.1)\n",
        "\n",
        "# run experiment and get the value functions from agent\n",
        "run_experiment(grid, agent, num_steps)\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "\n",
        "# visualise value functions\n",
        "print('AFTER {} STEPS ...'.format(num_steps))\n",
        "plot_action_values(q, epsilon=0.)\n",
        "\n",
        "# visualise the greedy policy\n",
        "plot_greedy_policy(grid, q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJastp_kcAZC",
        "colab_type": "text"
      },
      "source": [
        "### Task 2: Experiment with different levels of 'greediness':\n",
        "* The default was $\\epsilon=1$, what does this correspond to?\n",
        "* Try also $\\epsilon =0.1, 0.5$. What do you observe? Does the behaviour policy affect the training in any way?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omzJxb5ds0Iq",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 Experience Replay\n",
        "\n",
        " Implement an agent that uses **Experience Replay** to learn action values, at each step:\n",
        "* select actions randomly\n",
        "* accumulate all observed transitions *(s, a, r, s')* in the environment in a *replay buffer*,\n",
        "* apply an online Q-learning \n",
        "* apply multiple Q-learning updates based on transitions sampled from the *replay buffer* (in addition to the online updates).\n",
        "\n",
        "**Initialize** $Q(s, a)$ for all s ∈ S and a ∈ A(s)\n",
        "\n",
        "**Loop forever**:\n",
        "\n",
        "1. $S \\gets{}$current (nonterminal) state\n",
        " \n",
        "2. $A \\gets{} \\text{random_action}(S)$\n",
        " \n",
        "3. Take action $A$; observe resultant reward $R$, discount $\\gamma$, and state, $S'$\n",
        "\n",
        "4. $Q(S, A) \\gets Q(S, A) + \\alpha (R + \\gamma \\max_a Q(S, a) − Q(S, A))$\n",
        "\n",
        "5. $\\text{ReplayBuffer}.\\text{append_transition}(S, A, R, \\gamma, S')$\n",
        "\n",
        "6. Loop repeat n times:\n",
        "\n",
        "  1. $S, A, R, \\gamma, S' \\gets \\text{ReplayBuffer}.\\text{sample_transition}()$\n",
        "  \n",
        "  4. $Q(S, A) \\gets Q(S, A) + \\alpha (R + \\gamma \\max_a Q(S', a) − Q(S, A))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB9e_reb2pJX",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title [Coding Task] Q-learning AGENT with a simple replay buffer\n",
        "class ReplayQ_AGENT(object):\n",
        "\n",
        "  def __init__(\n",
        "      self, number_of_states, number_of_actions, initial_state, \n",
        "      behaviour_policy, num_offline_updates=0, step_size=0.1):\n",
        "    self._q = np.zeros((number_of_states, number_of_actions))\n",
        "    self._state = initial_state\n",
        "    self._action = 0\n",
        "    self._number_of_states = number_of_states\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._num_offline_updates = num_offline_updates\n",
        "    self._step_size = step_size\n",
        "    self._behaviour_policy = behaviour_policy\n",
        "    \n",
        "    # initialise replay buffer\n",
        "    self._replay_buffer = []\n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    return self._q\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    g = discount\n",
        "    next_s = next_state\n",
        "    \n",
        "    # Online Q-value update\n",
        "    td_error = r + g * np.max(self._q[next_s]) - self._q[s, a]\n",
        "    self._q[s, a] += self._step_size * td_error\n",
        "    \n",
        "    # ============ YOUR CODE HERE =============\n",
        "    if self._num_offline_updates > 0:\n",
        "\n",
        "      # Store sample into replay buffer memory\n",
        "      \n",
        "    \n",
        "      # ============ YOUR CODE HERE =============\n",
        "      # Q-value table update based on online sample and offline samples\n",
        "      # This update is the same as the above (Q-learning Agent) but \n",
        "      # now we are going to be using samples from the replay buffer.\n",
        "      # Note: You can COPY this from the above Q_learning Agent\n",
        "      pass\n",
        "\n",
        "    # Get the action to send to execute in the environment and return it\n",
        "    self._state = next_state\n",
        "    self._action = self._behaviour_policy(self._q[next_state])\n",
        "    \n",
        "    return self._action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5vnFSWVDU3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_offline_updates=30 #@param\n",
        "num_steps = int(1e4) #@param\n",
        "\n",
        "grid = Grid(discount=0.9, penalty_for_walls=-1.)\n",
        "\n",
        "behavior_policy = lambda qval: epsilon_greedy(qval, epsilon=epsilon)\n",
        "agent = ReplayQ_AGENT(\n",
        "    number_of_states=grid._layout.size, \n",
        "    number_of_actions=4, \n",
        "    initial_state=grid.get_obs(),\n",
        "    num_offline_updates=num_offline_updates, \n",
        "    step_size=0.1)\n",
        "\n",
        "# run experiment and get the value functions from agent\n",
        "run_experiment(grid, agent, num_steps)\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "\n",
        "# visualize value functions\n",
        "print('AFTER {} STEPS ...'.format(num_steps))\n",
        "plot_action_values(q)\n",
        "\n",
        "# visualise the greedy policy\n",
        "plot_greedy_policy(grid, q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5AWyVs16A-x",
        "colab_type": "text"
      },
      "source": [
        "## 1.5 Further Analysis:  Data Efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWutE_URvT7K",
        "colab_type": "text"
      },
      "source": [
        "**Online Q-learning**\n",
        "\n",
        "* $\\text{number_of_steps}$ = $1e3$ and $\\text{num_offline_updates}$ = $0$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iix-yw-MKS4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid = Grid()\n",
        "agent = ReplayQ_AGENT(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=0, step_size=0.1)\n",
        "run_experiment(grid, agent, int(1e3))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZXsXJYBgC_N",
        "colab_type": "text"
      },
      "source": [
        "**Experience Replay**\n",
        "\n",
        "* $\\text{number_of_steps}$ = $1e3$ and $\\text{num_offline_updates}$ = $30$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASml5uAeIl4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid = Grid()\n",
        "agent = ReplayQ_AGENT(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=30, step_size=0.1)\n",
        "run_experiment(grid, agent, int(1e3))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-YdF57kT5Tt",
        "colab_type": "text"
      },
      "source": [
        "# RL Lab - Part 2: REINFORCE Agent w/ FA \n",
        "\n",
        "## REINFORCE Agent\n",
        "\n",
        "We are still trying to solve the control problem: estimate the policy that gives us a better long term (discounted) return:\n",
        "\n",
        "$$G_t = \\sum_{k=t+1} \\gamma^{k-t-1}R_k $$\n",
        "\n",
        "Objective:\n",
        "$$J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}} [G_t]$$\n",
        "\n",
        "\n",
        "Using the log-derivative trick, we can obtain the gradient of $J(\\theta)$ with respect to the policy parameters $\\theta$ as:\n",
        "\n",
        "$$\\nabla_{\\theta} J \\propto \\mathbb{E}_{{\\pi_{\\theta}}}[\\sum_{t}\\nabla_{\\theta}\\log \\pi_{\\theta}(s_t,a_t) G_t]$$\n",
        "\n",
        "We estimate this gradient via sampling episodes in the enviroment.\n",
        "\n",
        "\n",
        "\n",
        "## Functional Approximation involved\n",
        "**Approximate** $ \\pi(s,a)$ for all s ∈ S and a ∈ A(s) via a functional approximator (in our case a sonnet/tensorflow model) and learn this function directly. In the second part we will introduce a baseline in the form a value function $V(s)$ that will be represented by an additional functional approximator.\n",
        "\n",
        "\n",
        "### Reference and Further Reading\n",
        "For futher information and refresher on this algorithm and general policy-gradient approached, please check out Chapter 13: Policy Gradient Methods in [Book](https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view). Models covered in this tutorial: \n",
        "\n",
        "13.3 REINFORCE: Monte Carlo Policy Gradient (2.1)\n",
        "\n",
        "13.4 REINFORCE with Baseline (2.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8b_qEawj35Y",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 (Vanilla) REINFORCE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU1aBWr4u4tc",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.1 Build agent\n",
        "This is slipt into two part:\n",
        "* Build the Policy functional approximator\n",
        "* Build the REINFORCE Agent\n",
        "\n",
        "Note: In the code below we will be using (by default) very simple transformations (linear layers) as this colab is design to focus on the RL algorithm. Please check out future colabs from the summer school for more reference on specifying more interesting functional approximation instances (convolutional nets, LSTM-s, etc) and feel free to experiment with other, more intricated networks. Whenever designing these, always try to keep in mind what are the properties and requirements for the functions you are trying to approximated -- e.g. whether or not you need longer term dependencies, or whether the current observation is sufficient to estimate the target values/intended transformations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvBpu_fbZtsL",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Helper code: Episode Logging\n",
        "# Book keeping of some useful (episode) statistics\n",
        "Logging_EpStats = namedtuple(\"Logging_EpStats\", \n",
        "                             [\"episode_lengths\", \"episode_rewards\"])   \n",
        "\n",
        "# We are going to be storing transitions encountered in the episode to use \n",
        "# later in the update (at the end of the episode, for REINFORCE)\n",
        "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"discount\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nFZqgPnne1U",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title [Coding Task] Policy Approximator\n",
        "#Define a class that build our policy approximation\n",
        "class PolicyApproximator():\n",
        "    \"\"\"\n",
        "    Policy Function approximator. \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_actions=4, learning_rate=0.01, scope=\"policy_approximation\"):\n",
        "        with tf.variable_scope(scope):\n",
        "            # This is a function of the state V(state)\n",
        "            self.state = tf.placeholder(tf.int32, [], \"state\")\n",
        "            \n",
        "            # Embbed the state into a one-hot coding\n",
        "            state_one_hot = tf.one_hot(self.state, int(grid._layout.size))\n",
        "            \n",
        "            # We are be evaluating the policy of a (previously) selected action\n",
        "            self.action = tf.placeholder(dtype=tf.int32, name=\"action\")\n",
        "            \n",
        "            # Return placeholder\n",
        "            self.return_s = tf.placeholder(dtype=tf.float32, name=\"return\")\n",
        "            \n",
        "            # Very simple (linear) transformation of the state to \n",
        "            # \\pi_{\\theta}(a|s) -- this is can be anything you think\n",
        "            # your solution class needs to span the intermediate sol.\n",
        "            self.output_layer = tf.layers.dense(state_one_hot,  num_actions,\n",
        "                                                activation=None)\n",
        "            \n",
        "\n",
        "            # ============ YOUR CODE HERE =============\n",
        "            # Compute pi(a=self.action|s) for a selected action\n",
        "            # self.action_probs =\n",
        "            \n",
        "            # Define the loss\n",
        "            # self.loss = \n",
        "            \n",
        "            # Define an optimizer\n",
        "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "            \n",
        "            # Define the training operation\n",
        "            self.train_op = self.optimizer.minimize(\n",
        "                self.loss, global_step=tf.contrib.framework.get_global_step())\n",
        "    \n",
        "    def predict(self, state, sess=None):\n",
        "      # ============ YOUR CODE HERE =============\n",
        "      sess = sess or tf.get_default_session()\n",
        "      # compute the probabilities of all actions given \n",
        "      # this state \\pi(a|s) for all a.\n",
        "      #pi_a_s = \n",
        "      return pi_a_s\n",
        "\n",
        "    def update(self, state, target, action, sess=None):\n",
        "      # ============ YOUR CODE HERE =============\n",
        "      sess = sess or tf.get_default_session()\n",
        "      # perform training/operation operation\n",
        "      # compute and return the loss\n",
        "      return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "senpTa9-p3FO",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title [Coding Task] REINFORCE AGENT\n",
        "class REINFORCE_AGENT(object):\n",
        "  \n",
        "  def __init__(\n",
        "      self, number_of_states, number_of_actions, initial_state, \n",
        "      policy_learning_rate=0.01, const_discount=1.0):\n",
        "    \n",
        "    self._policy_approximator = PolicyApproximator(num_actions=number_of_actions,\n",
        "                                                   learning_rate=policy_learning_rate)\n",
        "    self._constant_discount = const_discount \n",
        "    \n",
        "    # initial state/action\n",
        "    self._state = initial_state\n",
        "    self._action = 0\n",
        "    \n",
        "  def step(self, state):\n",
        "    # ============ YOUR CODE HERE =============\n",
        "    # Get the action probabilities \n",
        "    # action_probs = \n",
        "    \n",
        "    # Select you action\n",
        "    # action = \n",
        "    \n",
        "    # Update the internal variables\n",
        "    self._action = action\n",
        "    self._state = state\n",
        "    return self._action\n",
        "    \n",
        "  def update(self, episode):\n",
        "    # go over the all experience collected in this episode\n",
        "    for t, transition in enumerate(episode):\n",
        "      \n",
        "      # ============ YOUR CODE HERE =============\n",
        "      # Compute the (discounted) return\n",
        "      # discounted_return = \n",
        "      \n",
        "      # Update our policy estimator based on return\n",
        "      # self._policy_approximator.update(...)\n",
        "      \n",
        "      pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL9i62GGhZno",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title [IMPORTANT] Run REINFORCE agent with an environment 'env'\n",
        "# Description:\n",
        "# Simple experiment run loop (similar to the one above for tabular experiments)\n",
        "# -----------------------------------------------------------------------------\n",
        "# Expected behaviour \n",
        "# 1) For each episode repeat:\n",
        "#  - Interact with the environment (get observation and discount)\n",
        "#  - Store transition\n",
        "# 2) At the end of the episode, use the stored transition to update agent\n",
        "# Repeat for num_episode\n",
        "# -----------------------------------------------------------------------------\n",
        "# Additional: Log and return episode stastics for plotting later on\n",
        "# -----------------------------------------------------------------------------\n",
        "def run_reinforce(env, agent, num_episodes, \n",
        "                  MAXSTEPS_PER_EPISODE=100, \n",
        "                  REPORT_EVERY_N_STEPS=20):\n",
        "    \"\"\"\n",
        "    Run REINFORCE agent in a MDP especified by 'env'. \n",
        "    (Any agent that follows the same logic and can be plugged in though.)\n",
        "    \n",
        "    Agent requirements:\n",
        "      agent.step(state)\n",
        "      agent.update(episode)\n",
        "    \n",
        "    Enviroment requirements:\n",
        "      env.step(action)\n",
        "    \n",
        "    -----------------------------------------------------------------------\n",
        "    Inputs:\n",
        "        env: gridworld\n",
        "        agent: REINFORCE agent (or alternative)\n",
        "        num_episodes: Number of episodes to run for\n",
        "    \n",
        "    Returns:\n",
        "        Logging_EpStats: episode statistics (episode_length & episode_reward)\n",
        "    \"\"\"\n",
        "\n",
        "    # Book-keeping of some useful (episode) statistics\n",
        "    stats = Logging_EpStats(\n",
        "        episode_lengths=np.zeros(num_episodes),\n",
        "        episode_rewards=np.zeros(num_episodes))    \n",
        "    \n",
        "    for i_episode in range(num_episodes):\n",
        "      \n",
        "        # Reset the environment and pick the first action\n",
        "        action = 0#agent.initial_action()\n",
        "        reward, discount, next_state = env.step(action)\n",
        "        \n",
        "        episode = []\n",
        "        \n",
        "        # One step in the environment\n",
        "        state = next_state\n",
        "        for t in range(MAXSTEPS_PER_EPISODE): \n",
        "            \n",
        "            # Take a step\n",
        "            action = agent.step(state)\n",
        "            reward, discount, next_state = env.step(action)\n",
        "            \n",
        "            # Keep track of the transition\n",
        "            episode.append(Transition(\n",
        "              state=state, action=action, reward=reward, next_state=next_state, discount=discount))\n",
        "            \n",
        "            # Optional: Logging and reporting (live) statistics for this epsiode\n",
        "            \n",
        "            # Update statistics\n",
        "            stats.episode_rewards[i_episode] += reward\n",
        "            stats.episode_lengths[i_episode] = t\n",
        "            \n",
        "            # (Live) reporting\n",
        "            if ((discount == 0) | (t==MAXSTEPS_PER_EPISODE-1)) & (i_episode%REPORT_EVERY_N_STEPS==0):\n",
        "                # Print out which step we're on, useful for debugging\n",
        "                print(\"Episode {}/{}: Length {} ({})\".format(\n",
        "                i_episode + 1, num_episodes, t, stats.episode_rewards[i_episode - 1]))\n",
        "\n",
        "            if discount == 0: # this signals end of the episode          \n",
        "                break\n",
        "                \n",
        "            state = next_state\n",
        "    \n",
        "        # Go through the episode and make policy updates\n",
        "        agent.update(episode)\n",
        "      \n",
        "    return stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDAgK3pUvLuk",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.2. Run experiment\n",
        " Ready go! Let's test our agent!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2lyi-FZhZnr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialise an instance of the environment\n",
        "grid = Grid(discount=1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ3X6hZohZnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "\n",
        "agent = REINFORCE_AGENT(number_of_states=grid._layout.size,\n",
        "                        number_of_actions=4, \n",
        "                        initial_state=grid.get_obs(), \n",
        "                        policy_learning_rate=0.01, \n",
        "                        const_discount=1.0)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.initialize_all_variables())\n",
        "    # Note, due to randomness in the policy the number of episodes you need to learn a good\n",
        "    # policy may vary. ~500-1000 should be okay\n",
        "    stats = run_reinforce(grid, agent, 500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9w_ylGcjVnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_stats(stats, window=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOxqqWyPf9E5",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 (Optional) Adding a baseline (via $V(s)$) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfIkMhcDupVN",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.1. Build the Agent\n",
        "This is similar to the above agent, but now we have an additional problem of estimating/computing the value function $v(s)$.\n",
        "\n",
        "Your tasks will be:\n",
        "* Implement the value function approximator: prediction/update\n",
        "* Adjust the above agent to include this baseline \n",
        "(Note: make sure you remember to update both your value and policy approx.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTC91ChZjf34",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title [Coding Task] Value function approximator \n",
        "class ValueApproximator():\n",
        "    \"\"\"\n",
        "    Value Function approximator. \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, learning_rate=0.1, scope=\"value_approximation\"):\n",
        "        with tf.variable_scope(scope):\n",
        "          \n",
        "            # This is a function of the state V(state)\n",
        "            self.state = tf.placeholder(tf.int32, [], \"state\")\n",
        "            # Embbed the state into a one-hot coding\n",
        "            state_one_hot = tf.one_hot(self.state, int(grid._layout.size))\n",
        "            \n",
        "            # ============ YOUR CODE HERE =============\n",
        "            # Target Q-value function\n",
        "            # self.target = ...\n",
        "            \n",
        "            # Very simple (linear) transformation of the state to \n",
        "            # the value function V_{\\theta}(s) \n",
        "            # self.output_layer = ...\n",
        "            \n",
        "            self.value_estimate = tf.squeeze(self.output_layer)\n",
        "            \n",
        "            # ============ YOUR CODE HERE =============\n",
        "            # Define the loss\n",
        "            # self.loss = \n",
        "            \n",
        "            # Define an optimizer\n",
        "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "            \n",
        "            # Define the training operation\n",
        "            self.train_op = self.optimizer.minimize(\n",
        "                self.loss, global_step=tf.contrib.framework.get_global_step())        \n",
        "    \n",
        "    def predict(self, state, sess=None):\n",
        "      sess = sess or tf.get_default_session()\n",
        "      # ============ YOUR CODE HERE =============\n",
        "      # v_s = \n",
        "      return v_s\n",
        "\n",
        "    def update(self, state, target, sess=None):\n",
        "      sess = sess or tf.get_default_session()\n",
        "      # ============ YOUR CODE HERE =============\n",
        "      # loss =\n",
        "      return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gWaK0vctfoG",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title [Coding Task] REINFORCE with baseline via estimating V(s)\n",
        "class REINFORCE_AGENT(object):\n",
        "  \n",
        "  def __init__(\n",
        "      self, number_of_states, number_of_actions, initial_state, \n",
        "      policy_learning_rate=0.01, value_learning_rate=0.1, const_discount=1.0,\n",
        "      use_baseline=True):\n",
        "    \n",
        "    self._value_approximator  = ValueApproximator(learning_rate=value_learning_rate)\n",
        "    self._policy_approximator = PolicyApproximator(learning_rate=policy_learning_rate)\n",
        "    self._constant_discount = const_discount \n",
        "    \n",
        "    # initial state/action\n",
        "    self._state = initial_state\n",
        "    self._action = 0\n",
        "    self._use_baseline = use_baseline\n",
        "    \n",
        "  def step(self, state):\n",
        "    # ============ YOUR CODE HERE =============\n",
        "    # Note: you an COPY this from above\n",
        "    # Select the action to send to the environment\n",
        "    self._action = action\n",
        "    self._state = state\n",
        "    return self._action\n",
        "    \n",
        "  def update(self, episode):\n",
        "    \n",
        "    # go over the all experience collected in this episode\n",
        "    for t, transition in enumerate(episode):\n",
        "      # ============ YOUR CODE HERE =============\n",
        "      # Compute the discounted return\n",
        "      # discounted_return = \n",
        "      \n",
        "      if self._use_baseline:\n",
        "        # ============ YOUR CODE HERE =============\n",
        "        # Compute baseline/advantage\n",
        "        # baseline = \n",
        "        \n",
        "        # Compute advantage\n",
        "        # advantage = \n",
        "        \n",
        "        # Update our value estimator\n",
        "        # self._value_approximator.update(...)\n",
        "        \n",
        "        # Update our policy estimator\n",
        "        # self._policy_approximator.update(...)\n",
        "        pass\n",
        "      else:\n",
        "        # Update our policy estimator based on return\n",
        "        # self._policy_approximator.update(...)\n",
        "        pass\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k10bD7O9ujp4",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.2. Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JXfjIJYsGzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid = Grid(discount=1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsKIzTzY81_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "\n",
        "agent = REINFORCE_AGENT(number_of_states=grid._layout.size,\n",
        "                        number_of_actions=4, \n",
        "                        initial_state=grid.get_obs(), \n",
        "                        policy_learning_rate=0.01, \n",
        "                        value_learning_rate=0.1, \n",
        "                        const_discount=1.0,\n",
        "                        use_baseline=True)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.initialize_all_variables())\n",
        "    # Note, due to randomness in the policy the number of episodes you need to learn a good\n",
        "    # policy may vary. ~2000-5000 seemed to work well for me.\n",
        "    stats = run_reinforce(grid, agent, 500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFg1r7wlJp1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_stats(stats, window=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3GC9gSqYrJfB"
      },
      "source": [
        "## 2.3 (BONUS, Pen & paper) Policy Gradients and Partial Observability\n",
        "\n",
        "Consider a simple 2x2 gridworld.\n",
        "\n",
        "- The agent starts in one of the top cells.\n",
        "- Both cells on the bottom row are terminal.\n",
        "- The bottom left cell provides a negative -1 reward.\n",
        "- The bottom right cell provides a positive +1 reward.\n",
        "- There is a fixed discount of 0.9\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "6EB_E5npX9KM",
        "colab": {}
      },
      "source": [
        "#@title MDP rewards\n",
        "\n",
        "plt.figure(figsize=(3,0.7))\n",
        "clust_data = np.array([[0, 0], [-1, 1]])\n",
        "collabel=(\"col 1\", \"col 2\", \"col 3\")\n",
        "the_table = plt.table(cellText=clust_data, loc='center')\n",
        "plt.axis('tight')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qf8XzXLbtW3K"
      },
      "source": [
        "### Question 2.3.1.\n",
        "\n",
        "\n",
        "What are the *optimal action values*  if the agent can perceive exactly in what state it is?\n",
        "\n",
        "\n",
        "### Question 2.3.2\n",
        "\n",
        "What are the *optimal action values*  if the agent cannot tell whether it is in the right or left column?\n",
        "\n",
        "\n",
        "\n",
        "### Question 2.3.3\n",
        "\n",
        "What is the optimal policy if the agent cannot tell whether he is in the right or left column?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nvK1SjRj3bp",
        "colab_type": "text"
      },
      "source": [
        "### That's a wrap! \n",
        "Thank you for going through this tutorial and we hope you found this useful and/or informative. Remember though this is only a starting point, but hopefully one that would inspire more thought and more experimentation in this domain.  As seen throughout this tutorial, 'trial-and-error' can be a very powerful learning mechanism.\n",
        "\n",
        "\n",
        "\"*Negative results are just what I want. They’re just as valuable to me as positive results. I can never find the thing that does the job best until I find the ones that don’t.*” (Thomas Edison)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js3mwPaOmR3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}w